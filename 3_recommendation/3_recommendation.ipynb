{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple recommender system\n",
    "\n",
    "In this notebook we will see a simple implementation of a Recommender System based on *Collaborative Filtering*. We will use the MovieLens dataset:\n",
    "\n",
    "https://grouplens.org/datasets/movielens/ -> it has different sources of datasets of different sizes (we use the 1 milions dataset, but you can use others to check if everything works)\n",
    "\n",
    "Since we will store the full utility matrix, we consider a small dataset, and in particular the one recommended for education and development (small version). Such a dataset contains approximately 100,000 ratings to 9,000 films made by 600 users.\n",
    "\n",
    "Users and items (hereinafter, we will use the term \"item\" instead of \"film\") are identified by integers, and the rating is a number bewteen 1 and 5. A sample of the file, which contains as also the timestamps, is:\n",
    "\n",
    "```text\n",
    "userId, movieId, rating, timestamp\n",
    "1,1,4.0,964982703\n",
    "1,3,4.0,964981247\n",
    "1,6,4.0,964982224\n",
    "1,47,5.0,964983815\n",
    "1,50,5.0,964982931\n",
    "1,70,3.0,964982400\n",
    "1,101,5.0,964980868\n",
    "```\n",
    "We will not use timestamps, but it could be useful for analysis of recent ratings rather than old ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "\n",
    "We first define the function to load the data. As usual, we need to adapt such a function to the specific file input format.\n",
    "\n",
    "In particular, we are going to assign to users and items progressive numbers, so their identifications will be also the indexes of the utility matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex, since the userId is the index of the matrix (starting from zero): it's not sure that the id of users or film is sequential,\n",
    "# since the dataset is already \"made\"!\n",
    "# users = rows, films/items = columns\n",
    "# If users are not in sequential order -> e.g. 1, 2, 5, 83 -> need to translate the userId with the corresponding index, creating a dictionary of\n",
    "# users where userid is the key and index of the matrix is the value \n",
    "# Same is done for the item -> map the ids as the identifier of the matrix\n",
    "def load_data(filename):\n",
    "    input_lines = [] # before creating the matrix I creat a list of lists with the indexes of the matrix and ratings -> will read it to create the matrix (since idk a priori its size)\n",
    "    users = {} # empty dict\n",
    "    num_users = 0\n",
    "    items = {} # empty dict\n",
    "    num_items = 0\n",
    "    raw_lines = open(filename, 'r').read().splitlines() # open file and reach each line\n",
    "    # Remove the first line since it's a description\n",
    "    del raw_lines[0] \n",
    "    for line in raw_lines: # read line by line\n",
    "        line_content = line.split(',') # line = text, separated by comma\n",
    "        user_id = int(line_content[0]) # first element is user identifier\n",
    "        item_id = int(line_content[1]) # second element is item identifier\n",
    "        rating = float(line_content[2]) # thirs element is rating (a float since later we will normalize, so it's easier to have a float from the beginner)\n",
    "        # Transalte into matrix indexes\n",
    "        if user_id not in users:\n",
    "            users[user_id] = num_users # add user\n",
    "            num_users += 1 # increase counter, initialized as 0 (since index of matrix)\n",
    "        if item_id not in items:\n",
    "            items[item_id] = num_items\n",
    "            num_items += 1\n",
    "        input_lines.append([users[user_id], items[item_id], rating]) # it's my list of lists\n",
    "    return input_lines, num_users, num_items # list, number of users and items "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file containing the dataset is called \"3-ratings.csv\".\n",
    "\n",
    "On Colab, remember to mount your Drive\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "input_file = \"/content/drive/My Drive/...\"\n",
    "```\n",
    "Let's load our dataset and see its initial content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset contains 610 users, 9724 items, and 100836 ratings.\n",
      "\n",
      "The first five ratings are: [[0, 0, 4.0], [0, 1, 4.0], [0, 2, 4.0], [0, 3, 5.0], [0, 4, 5.0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = \"./3_ratings.csv\"\n",
    "\n",
    "input_ratings, num_users, num_items = load_data(input_file)\n",
    "\n",
    "print(\"\\nThe dataset contains\", num_users, \"users,\", \n",
    "      num_items, \"items, and\", len(input_ratings),\"ratings.\\n\")\n",
    "print(\"The first five ratings are:\", input_ratings[:5],\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility matrix\n",
    "\n",
    "From list to utility matrix <- since it's sparse, we usually work with list (or we waste space storing zeros). Since the dataset is small we work with matrixes, because it's easier. Building and maintaining in memory the utility matrix is inefficient, since the matrix is sparse. But working with a matrix is more intuitive, therefore we will use such an approach. \n",
    "\n",
    "The best way to handle a matrix and the operations associated to it is to use NUMPY arrays. If you are not familiar with Numpy, you can find different tutorials online. See for instance:\n",
    "\n",
    "https://www.kaggle.com/saptarsi/numpy-tutorial-notebook-sg  \n",
    "https://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1.70%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4. , 4. , 4. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       ...,\n",
       "       [2.5, 2. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [3. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [5. , 0. , 5. , ..., 3. , 3.5, 3.5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # use numpy to create the utility matrix\n",
    "\n",
    "# I know the size of my matrix -> can create an NxI matrix of zeros, where N = number of users\n",
    "# and I = number of items. I will then fill the above matrix with the correct values\n",
    "ratings = np.zeros((num_users, num_items)) # pass 2 numbers to create a matrix (number of rows and columns)\n",
    "\n",
    "# Fill the matrix with the ratings \n",
    "for row in input_ratings: # go through my list\n",
    "    ratings[row[0], row[1]] = row[2] # take first element as index of row and second as index of column, filling the cell with the third element\n",
    "\n",
    "# Once I filled the matrix, I compute the \"sparsity\", i.e., percentage of non-zero cells\n",
    "sparsity = 100*float(np.count_nonzero(ratings))/float(num_users*num_items) # this works no matter the size of the matrix\n",
    "# I know my matrix has 610 rows and 9724 columns + 100836 elements (not zeros)-> so I could have done sparsity = 100836/(610x9724)\n",
    "print(\"Sparsity: %.2f%%\\n\" % (sparsity))\n",
    "\n",
    "# Show a snippet of the matrix\n",
    "ratings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary analysis\n",
    "-> stat analysis on the matrix\n",
    "\n",
    "In the following, we suggest a set of preliminary analysis on the dataset that can be carried out with simple operations on the matrix:\n",
    "\n",
    "### Question  Q1\n",
    "<div class=\"alert alert-info\">\n",
    "For a given user id, find the number of rated items by that user, and the average rating.  \n",
    "    \n",
    "- **Hint**: Compute these values once for all users, and store them in another Nx2 matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "216\n",
      "3.5555555555555554\n"
     ]
    }
   ],
   "source": [
    "# Count how many items the user has rated and the average rating -> gives an idea of activity of user/low or high rating from a user\n",
    "\n",
    "# Select a specific cell\n",
    "print(ratings[3, 6]) # user 3 for movie 6\n",
    "# Count how many movies an user rated\n",
    "ratings[3, :] # select the whole row corresponding to user 3\n",
    "print(np.count_nonzero(ratings[3, :]))\n",
    "# Repeat this for all users!\n",
    "\n",
    "# Print the average rating of an user\n",
    "print(np.sum(ratings[3, :])/np.count_nonzero(ratings[3, :]))\n",
    "\n",
    "# Result in anoher matrix, same row as the utility but only 2 columns: number of items rated by that user and their average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix with users and items -> compute foreach users the number of items rated and the average rating\n",
    "# FOTOOOOOOOOOOOOOOOOOOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q2\n",
    "<div class=\"alert alert-info\">\n",
    "Find the top-k viewers, i.e., users that rated the highest number of items.\n",
    "    \n",
    "- *Variation*: find the bottom-k viewers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I sort the result above\n",
    "\n",
    "# I want the users who are more active -> their standard deviation tend to be lower than the one of inactive users\n",
    "# k can be 10 or 20, for example\n",
    "# Since above I have a numpy array, I focus on the first column and sort it with argsort (it creates an array f index from highest to lowest)\n",
    "# Use negative since sot is increasing and I want decreasing\n",
    "# FOTOOOOOOOOOOOO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average is very different between users!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q3\n",
    "<div class=\"alert alert-info\">\n",
    "For a given item id, find the number of users that rated it, and its average rating.\n",
    "    \n",
    "- **Hint**: Compute these values once for all items, and store them in another Ix2 matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "23\n",
      "3.782608695652174\n"
     ]
    }
   ],
   "source": [
    "# Same as Q1, but for items\n",
    "\n",
    "print(ratings[3, 6])\n",
    "ratings[:, 6]\n",
    "print(np.count_nonzero(ratings[:, 6]))\n",
    "# Repeat this for all items!\n",
    "\n",
    "\n",
    "# Print the average rating of an item\n",
    "print(np.sum(ratings[:, 6])/np.count_nonzero(ratings[:, 6]))\n",
    "\n",
    "\n",
    "# Result is a vector: same number of items, for each the number of users who rated it and its average rating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tells me the number of rating of a movie and its avergae rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q4\n",
    "<div class=\"alert alert-info\">\n",
    "Find the top-k items with at least v views, i.e., the k items with the highest average rate that has been rated by at least v users.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as Q2, but for items\n",
    "# I filter out the items with not enough views (with a boolean mask that create a boolean matrix) and then do it                                                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q5\n",
    "<div class=\"alert alert-info\">\n",
    "Normalize the utility matrix by subtracting from each non-zero cell at row i the average rating of the user i.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average from user/item POV -> can compute two distinct Normalized Utility matrixes\n",
    "# Normalize user = take their average rating and substract that value from each positive cell (consider only the cell with values >0,\n",
    "# since =0 are movies not rated by the user)\n",
    "\n",
    "# Can be done by the user or item POV. If we don't normalize, the error can be very high in the Collaborative Filtering method! With\n",
    "# normlization, predictions become more precise\n",
    "\n",
    "# Create empty matrix, in which I copy the normalized values only for items > 0 (= they have a rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the utility matrix\n",
    "\n",
    "We want to separate the values of utility matrix into two sets, train and test:\n",
    "\n",
    "- The train set is used to compute the similarity between users or items;\n",
    "- Using the similarity, we will then predict the ratings;\n",
    "- We compare the prediction with the values in the test set to compute the prediction error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Previous definition, SKIP IT\n",
    "def train_test_split_old(ratings, sample_per_user=10):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=sample_per_user, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split matrix = create another matrix of same dimension, take ratings from input matrix and copy in the new -> set them to 0 in\n",
    "# the input matrix (as if they had not been scored)\n",
    "def train_test_split(ratings, sample_per_user=10, seed = 123425536):\n",
    "    test = np.zeros(ratings.shape) # Create new empty matrix, same size as the input matrix\n",
    "    # .shape = number of rows and columns\n",
    "    train = ratings.copy() # Make a copy of ratings so I don't work directly on the dataset\n",
    "    # For each user I take 10% of their ratings (so it's fair for active and inactive users) -> want a random set of those ratings\n",
    "    # -> use seed so the experiment can be done again\n",
    "    np.random.seed(seed)\n",
    "    for user in range(ratings.shape[0]): # go through all rows in rating\n",
    "        # Count the number of non zero elements \n",
    "        # count_nonzero count the number of non zero elements\n",
    "        # .nonzero create a datastructure that contains the indexes of non zero elements from the vector made up of the user ratings:\n",
    "        # create a list of lists, in which I care only about the first element (that's why [0]) since I'm working on vectors\n",
    "        num_ratings = len(ratings[user, :].nonzero()[0]) # get the dimension of the vector: number of ratings I'm using\n",
    "        # If it's 0, then user has no ratings -> all zeros\n",
    "        if num_ratings == 0:\n",
    "            continue\n",
    "        # If there are ratings, I count them and take a percentage of them\n",
    "        actual_sample = int(num_ratings*sample_per_user/100)\n",
    "        # Take first element of a random permutation (random.choice) without replacement -> same as selecting randomly ratings \n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=actual_sample, \n",
    "                                        replace=False)\n",
    "        # Use index to set to zero the ratings in the input matrix\n",
    "        train[user, test_ratings] = 0.\n",
    "        # Copy the values from the original matrix (not the copy, I already edited it)\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the above function by using 15 samples from each user for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero elements in ratings  100836\n",
      "Non-zero elements in train  91018\n",
      "Non-zero elements in test  9818\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(ratings, sample_per_user=10) # use 10% of the ratings for testing\n",
    "\n",
    "print(\"Non-zero elements in ratings \", np.count_nonzero(ratings))\n",
    "print(\"Non-zero elements in train \", np.count_nonzero(train))\n",
    "print(\"Non-zero elements in test \", np.count_nonzero(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% in train and 10% in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version can be used after having answered to Q5\n",
    "\n",
    "train_u, test_u = train_test_split(ratings_norm_u, sample_per_user=10) # normalized over the user\n",
    "train_i, test_i = train_test_split(ratings_norm_i, sample_per_user=10) # normalized over the item"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the similarity matrix \n",
    "MIN 52??????????????\n",
    "\n",
    "Using the train set, we compute the user-user similarity matrix (NxN) and the item-item similairyt matrix (IxI). From the mathematical point of view, we have for users x and y:\n",
    "\n",
    "$$\n",
    "sim(x,y) = \\cos(r_x, r_y) = \\frac{\\sum_i r_{xi} r_{yi}}{\\sqrt{\\sum_i r_{xi}^2}\\sqrt{\\sum_i r_{yi}^2}}\n",
    "$$\n",
    "It results in the similariy matrix (which is square): 1s in the diagonal and then the values of simil between user x and y. I use the cosine similarity, since it's the correlation over the normalized values.\n",
    "\n",
    "The denominator is the norm!\n",
    "\n",
    "A similar computation can be done for the item-item similarity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each matrix is computed using matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on user-user\n",
    "# Take the rating matrix and transpose it: now I can simply do a matrix multiplication\n",
    "def compute_similarity(ratings, kind='user', epsilon=1e-9):\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        # epsilon is for computational POV, since there are lot of zeros -> simil between 2 users could be 0 and\n",
    "        # computing the norm takes a division by 0, which can be a problem -> add small epsilon to solve it\n",
    "        sim = ratings.dot(ratings.T) + epsilon # simil = multipl of 2 matrixes\n",
    "    elif kind == 'item':\n",
    "        sim = ratings.T.dot(ratings) + epsilon\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))]) # diagonal is the product of vector to itself -> it's squared already\n",
    "    # norms is a vector\n",
    "    return (sim / norms / norms.T) # this is the sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to compute the two matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.24628441 0.32583414 0.39350118]\n",
      " [0.24628441 1.         0.21485671 0.23276016]\n",
      " [0.32583414 0.21485671 1.         0.44114166]\n",
      " [0.39350118 0.23276016 0.44114166 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "user_similarity = compute_similarity(train, kind='user')\n",
    "item_similarity = compute_similarity(train, kind='item')\n",
    "\n",
    "# Show the first values of the item-item similairty matrix\n",
    "print(item_similarity[:4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this version can be used after having answered to Q5\n",
    "\n",
    "user_similarity_n = compute_similarity(train_u, kind='user')\n",
    "item_similarity_n = compute_similarity(train_i, kind='item')\n",
    "\n",
    "# Show the first values of the item-item similairty matrix\n",
    "print(item_similarity_n[:4, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the prediction and the prediction error\n",
    "\n",
    "Using the similarity matrix, we predict the rating of missing values. In case of user-user similarity, if we want to predict item i for user x, we have:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{xi} = \\frac{\\sum_y sim(x,y) r_{yi}}{\\sum_y sim(x,y)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_simple(ratings, similarity, kind='user'):\n",
    "    if kind == 'user':\n",
    "        return similarity.dot(ratings) / np.array([similarity.sum(axis=1)]).T # similarity matrix x ratings matrix\n",
    "    elif kind == 'item':\n",
    "        return ratings.dot(similarity) / np.array([similarity.sum(axis=1)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the error (= how good is my method), we use the mean square error from Sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the test matrix I have the values used for my prediction -> I consider just them\n",
    "# ???? 1.11.30 FINO FINE\n",
    "# Once I made my prediction, the train matrix is filled with the predicted values -> take these elemnts and put them in pred ???\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten() # actual rating stored in test\n",
    "    actual = actual[actual.nonzero()].flatten() # take non zero values from ???\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE:  10.14829318830978\n",
      "Item-based CF MSE:  11.1352103623954\n"
     ]
    }
   ],
   "source": [
    "user_prediction = predict_simple(train, user_similarity, kind='user') # from train I predict based on user-user similairty\n",
    "item_prediction = predict_simple(train, item_similarity, kind='item')\n",
    "\n",
    "print('User-based CF MSE: ', get_mse(user_prediction, test))\n",
    "print('Item-based CF MSE: ', get_mse(item_prediction, test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE are proportional to the ratings (from 1 to 5), so an error of 10 is high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version can be used after having answered to Q5\n",
    "\n",
    "user_prediction_n = predict_simple(train_u, user_similarity_n, kind='user')\n",
    "item_prediction_n = predict_simple(train_i, item_similarity_n, kind='item')\n",
    "\n",
    "print('User-based CF MSE: ', get_mse(user_prediction_n, test_u))\n",
    "print('Item-based CF MSE: ', get_mse(item_prediction_n, test_i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item-item similarity is slightly better than user-user similarity.\n",
    "\n",
    "An error of 1 is a good start, a basic Collaborative Filtering. It's high since it's computed over all ratings (high and low) -> go to the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional questions\n",
    "\n",
    "The above procedure computes the similairity and the prediction for all users or all items. It would be interesting to work on single users, and see if we can improve the error for that user.\n",
    "\n",
    "### Question  Q6\n",
    "<div class=\"alert alert-info\">\n",
    "Consider the user similarity matrix and the item similarity matrix. \n",
    "For a given user id, consider the ratings in the test set. Predict those ratings using the user-user similarity matrix or with the items-items similairty matrix, and compute the error.\n",
    "    \n",
    "- **Note**: We are not interested in the error computed over all users, but only over for a specific user.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q7\n",
    "<div class=\"alert alert-info\">\n",
    "Repeat the above computation, but consider the error for only the top-5 recommended items.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus only on the high ratings of a specifi user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q8\n",
    "<div class=\"alert alert-info\">\n",
    "Repeat the computations for questions Q5 and Q6, but, as recommendation, consider the top 30 most similar users or items, and check if this has an impact on the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I computed the simil also with users with low similarity, now I focus only on the top similar users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
