{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation with Pandas\n",
    "\n",
    "Most of the datasets we are dealing with can be considered \"semi-structured\", i.e., even if they do not conform to a data model, they have an identifiable structure and they have some organisational properties that make it easier to analyze. Nevertheless, they can be still considered as **raw data**, since they may contain errors, missing values, or partial information. \n",
    "\n",
    "Examples of these datasets are:\n",
    "- logs produced by a web server, containing the user activity (e.g., clicks);\n",
    "- records produced by a network traffic analyzer, that monitors the network activities and produces summaries (e.g., packet exchange);\n",
    "- emails that are analyzed by a intrusion detection system to understand if there is an attack.\n",
    "\n",
    "Semi-structured data can be organized into a set of information with labels or tags, or in a tabular form (which can be the output of a pre-processing). \n",
    "\n",
    "In this notebook we will analyze a semi-structured dataset in a tabular form with the help of **Pandas**, which is a Python library for data manipulation and analysis. Students not familiar with Pandas may find some introductory tutorials here:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html  \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/cookbook.html  \n",
    "https://github.com/jvns/pandas-cookbook\n",
    "\n",
    "In the following, we first introduce our dataset, and then we show some common exploratory data analysis that is done on raw datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset - Airline data\n",
    "\n",
    "We consider the data related to airline activities that can be found at  \n",
    "http://stat-computing.org/dataexpo/2009/the-data.html\n",
    "\n",
    "It contains 29 fields, that can be either categorical or numerical. For example, the ```Origin``` (source airport) is categorical. The ```DepTime``` (departure time) feature is numerical. For instance, \"flight departing before 6PM\" can be express by \"DepTime < 1800\". The structure of each line is as follows:\n",
    "\n",
    "```\n",
    "1\t Year\t1987-2008\n",
    "2\t Month\t1-12\n",
    "3\t DayofMonth\t1-31\n",
    "4\t DayOfWeek\t1 (Monday) - 7 (Sunday)\n",
    "5\t DepTime\tactual departure time (local, hhmm)\n",
    "6\t CRSDepTime\tscheduled departure time (local, hhmm)\n",
    "7\t ArrTime\tactual arrival time (local, hhmm)\n",
    "8\t CRSArrTime\tscheduled arrival time (local, hhmm)\n",
    "9\t UniqueCarrier\tunique carrier code\n",
    "10\t FlightNum\tflight number\n",
    "11\t TailNum\tplane tail number\n",
    "12\t ActualElapsedTime\tin minutes\n",
    "13\t CRSElapsedTime\tin minutes\n",
    "14\t AirTime\tin minutes\n",
    "15\t ArrDelay\tarrival delay, in minutes\n",
    "16\t DepDelay\tdeparture delay, in minutes\n",
    "17\t Origin\torigin IATA airport code\n",
    "18\t Dest\tdestination IATA airport code\n",
    "19\t Distance\tin miles\n",
    "20\t TaxiIn\ttaxi in time, in minutes\n",
    "21\t TaxiOut\ttaxi out time in minutes\n",
    "22\t Cancelled\twas the flight cancelled?\n",
    "23\t CancellationCode\treason for cancellation (A = carrier, B = weather, C = NAS, D = security)\n",
    "24\t Diverted\t1 = yes, 0 = no\n",
    "25\t CarrierDelay\tin minutes\n",
    "26\t WeatherDelay\tin minutes\n",
    "27\t NASDelay\tin minutes\n",
    "28\t SecurityDelay\tin minutes\n",
    "29\t LateAircraftDelay\tin minutes\n",
    "```\n",
    "\n",
    "There is a single CSV file per year. Each file contains a header that serves the purpose of an \"embedded schema\", to help data scientist figure out what information is available. This is what the beginning of a file looks like:\n",
    "\n",
    "```\n",
    "Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\n",
    "1994,1,7,5,858,900,954,1003,US,227,NA,56,63,NA,-9,-2,CLT,ORF,290,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
    "1994,1,8,6,859,900,952,1003,US,227,NA,53,63,NA,-11,-1,CLT,ORF,290,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
    "1994,1,10,1,935,900,1023,1003,US,227,NA,48,63,NA,20,35,CLT,ORF,290,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
    "1994,1,11,2,903,900,1131,1003,US,227,NA,148,63,NA,88,3,CLT,ORF,290,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
    "1994,1,12,3,933,900,1024,1003,US,227,NA,51,63,NA,21,33,CLT,ORF,290,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
    "1994,1,13,4,NA,900,NA,1003,US,227,NA,NA,63,NA,NA,NA,CLT,ORF,290,NA,NA,1,NA,0,NA,NA,NA,NA,NA\n",
    "```\n",
    "\n",
    "Note that there are some fields with missing values in some lines of the dataset. The missing values are marked by \"NA\". These values can cause problems when processing the data and can lead to unexpected results. \n",
    " \n",
    "When analyzing the dataset, there are many intial questions that could be of insterest,  such as:\n",
    "\n",
    " - How many unique origin airports?\n",
    " - How many unique destination airports?\n",
    " - How many carriers?\n",
    " - How many night flights do we have in our data? (\"night\" starts at 6pm)\n",
    " - How many night flights per unique carrier?\n",
    " - ...\n",
    " \n",
    "In order to answer to such questions, we need to check our dataset and remove the causes of potential problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and initial exploration\n",
    "\n",
    "To understand if the dataset needs to be processed, a convenient approach is to consider a **random sample**. This will provide an indication about the quality of the dataset and, if there are some operations to do (e.g., remove lines with some missing values), the sample will provide a way to understand the *pattern* to follow.\n",
    "\n",
    "There are different ways for producing a sample, in our case we will work on a single year - an alternative approach would be to sample a set of lines from the different years.\n",
    "\n",
    "The dataset is organized as csv file, and Pandas has a method to load such a type of file (it considers the first row as the header that contains the name of the fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "# We read directly the compressed file\n",
    "input_file = \"7-1994.csv.gz\"\n",
    "dataset = pd.read_csv(input_file, compression='gzip')\n",
    "\n",
    "# Check how many rows and columsn the dataset contains\n",
    "num_rows, num_cols = dataset.shape\n",
    "print(\"The dataset contains\", num_rows, \"rows and\", num_cols, \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the first 10 rows (for ease of reading, we show the transpose, so that the columns becomes rows) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain additional information about each column with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas compute some basic statistics on the values in each column with ```.describe()``` - also in this case, we show the transpose for ease of reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the columns\n",
    "\n",
    "We start looking for columns that contains missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_miss = dataset.isna().sum()\n",
    "\n",
    "# Filtering only the columns with at least 1 missing value\n",
    "columns_with_miss = columns_with_miss[columns_with_miss!=0]\n",
    "print('Columns with missing values:', len(columns_with_miss))\n",
    "\n",
    "# Show the columns with missing values, sorted in descending order\n",
    "columns_with_miss.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there are 10 columns that do not contain any value (recall, the dataset has 5180048 rows), so we can remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=['LateAircraftDelay', 'SecurityDelay', \\\n",
    " 'NASDelay', 'WeatherDelay', 'CarrierDelay', 'CancellationCode', \\\n",
    " 'TaxiOut', 'TaxiIn', 'AirTime', 'TailNum'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take care of the other coulmns with missing values (```ArrDelay```, ```ActualElapsedTime```, ...) when we will analyze the rows.\n",
    "\n",
    "Besides missing values, we have also values equal to zero, so we need to check them and see if their values make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isin([0]).sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of them makes sense, except for ```ActualElapsedTime```, since it is stange that the flight time is equal to zero. We will consider it when analyzing the rows.\n",
    "\n",
    "We conclude this column-wise exploration by looking (visually) for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = \\\n",
    "list(dataset.dtypes[dataset.dtypes == 'int64'].index) + \\\n",
    "list(dataset.dtypes[dataset.dtypes == 'float64'].index)\n",
    "\n",
    "len(numerical_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "rows = 4\n",
    "columns = 4\n",
    "\n",
    "fig, axes = plt.subplots(rows,columns, figsize=(30,30))\n",
    "\n",
    "x, y = 0, 0\n",
    "\n",
    "for i, column in enumerate(numerical_col):\n",
    "    sns.boxplot(x=dataset[column], ax=axes[x, y])\n",
    "    \n",
    "    if y < columns-1:\n",
    "        y += 1\n",
    "    elif y == columns-1:\n",
    "        x += 1\n",
    "        y = 0\n",
    "    else:\n",
    "        y += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some outlier in ```Distance```, in which case we should analyze the specific rows and, in case, remove them (we leave this task to the student).\n",
    "\n",
    "## Analysis of the rows\n",
    "\n",
    "We now inspect the rows that we identified as suspicious during the general analysis of the columns, i.e., rows that contains NA or 0.\n",
    "\n",
    "For instance, we have seen that there are 11 rows in which the field ```ActualElapsedTime``` is zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['ActualElapsedTime'].isin([0])].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is clearly due to an error in recording the departure and arrival times. We could adjust the values, since we have the scheduled departure and scheduled arrival times, along with the departure and arrival delays. But since there are only 11 records, it's faster to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_drop =  dataset[dataset['ActualElapsedTime'].isin([0])].index\n",
    "dataset = dataset.drop(index_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, we can inspect the rows with NA in the fields \n",
    "\n",
    "|Field |Count|\n",
    "|:-----|----:|\n",
    "|ArrDelay               |78846|\n",
    "|ActualElapsedTime      |78846|\n",
    "|ArrTime                |78846|\n",
    "|DepDelay               |66740|\n",
    "|DepTime                |66740|\n",
    "|Distance               |22949|\n",
    "\n",
    "Notice how some counts are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['ActualElapsedTime'].isna()].head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For security, we remove the rows in which both ```ActualElapsedTime``` and ```ArrTime``` is NA, since without these two values we could not recover the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_drop =  dataset[dataset['ArrTime'].isna() & dataset['ActualElapsedTime'].isna()].index\n",
    "dataset = dataset.drop(index_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check if there are still NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Records with NA in ActualElapsedTime:\", dataset['ActualElapsedTime'].isna().sum())\n",
    "print(\"Records with NA in ArrDelay:\", dataset['ArrDelay'].isna().sum())\n",
    "print(\"Records with NA in ArrTime:\", dataset['ArrTime'].isna().sum())\n",
    "print(\"Records with NA in DepDelay:\", dataset['DepDelay'].isna().sum())\n",
    "print(\"Records with NA in DepTime:\", dataset['DepTime'].isna().sum())\n",
    "print(\"Records with NA in Distance:\", dataset['Distance'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As exercise, check if the records with ```Distance``` equal to NA should be dropped.\n",
    "\n",
    "## Data normalization\n",
    "\n",
    "Once the data has been cleaned (including the removal of outliers), we can look at the distribution of the numerical values and see if we need to normalize them. **In this specific dataset, normalization is not required**, since fields refer to times and distances. But we will normalize the values of a column as representative example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the empirical distribution of the field \"ActualElapsedTime\"\n",
    "dataset['ActualElapsedTime'].plot.hist(bins=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize with respect to the maximum value (in this case, we make a copy of the column beacuse we do no want to modify the dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AET_copy = dataset['ActualElapsedTime'].copy()\n",
    "AET_copy = AET_copy/AET_copy.max()\n",
    "AET_copy.plot.hist(bins=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can normalize with respect to the minimum and maximum values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AET_copy = dataset['ActualElapsedTime'].copy()\n",
    "AET_copy = (AET_copy - AET_copy.min())/(AET_copy.max() - AET_copy.min())\n",
    "AET_copy.plot.hist(bins=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to compute the z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AET_copy = dataset['ActualElapsedTime'].copy()\n",
    "AET_copy = (AET_copy - AET_copy.mean())/AET_copy.std()\n",
    "AET_copy.plot.hist(bins=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final considerations\n",
    "\n",
    "Once the dataset has been processed, we can save it with the command:\n",
    "```\n",
    "dataset.to_csv('1994-cleaned.csv')\n",
    "```\n",
    "\n",
    "Clearly, there are other pre-processing steps that can be done (e.g., check that the flight time is indeed the difference between the arrival and departure time), but such steps are specific to the dataset to be analyzed. In this short notebook, we have simply shown come general approach to data preparation, with common steps usualy adopted in such a process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
