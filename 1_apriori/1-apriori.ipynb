{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding frequent itemsets with the Apriori algorithm\n",
    "\n",
    "In this notebook we will see a simple implementation of the Apriori algorithm using only Python data types (*list, dictionary, tuple*).\n",
    "\n",
    "We assume that:\n",
    "- The item identifiers are integers;\n",
    "- The input file contains a set of baskets, one basket per line;\n",
    "- each item within a basket is separated by comma \",\"\n",
    "\n",
    "An example is:\n",
    "```text\n",
    "1,2,3,4\n",
    "1,2,5\n",
    "4,6,8,10,22,16\n",
    "```\n",
    "\n",
    "The other required input is the **support**, expressed as fraction of the total number of baskets (since we do not know how many baskets our dataset will contain).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "\n",
    "We first define the function to load the data.\n",
    "\n",
    "We put each line in a list, so the whole dataset will be a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    input_lines = []\n",
    "    raw_lines = open(filename, 'r').read().splitlines()\n",
    "    for line in raw_lines:\n",
    "        input_lines.append([int(x) for x in line.split(',')])\n",
    "    return input_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file containing the dataset is called \"1-baskets.txt\".\n",
    "\n",
    "Run the following cell if you are using Colab and you want to mount your google drive as data repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you then want to read a file, then call:\n",
    "```python\n",
    "input_file = \"/content/drive/My Drive/...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our dataset and print the first 5 lines (baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_file = \"[PATH]/1-baskets.txt\"\n",
    "input_file = \"./1-baskets.txt\"\n",
    "\n",
    "dataset = load_data(input_file)\n",
    "\n",
    "for elem in dataset[:4]:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First pass\n",
    "\n",
    "In the first pass, we filter the frequet single items.\n",
    "\n",
    "We will use a dictionary to keep track of the items and their counts -- this is the complete set of items, called **C1**. We then we filter the dictionary to remove items not sufficently frequent -- in this way we obtain the frequent items **L1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createL1(dataset, support):\n",
    "    freq_items = {}\n",
    "    for basket in dataset:\n",
    "        for item in basket:\n",
    "            if item in freq_items:\n",
    "                freq_items[item] += 1\n",
    "            else:\n",
    "                freq_items[item] = 1\n",
    "\n",
    "    # remove non frequent items; len(data) returns the number of baskets\n",
    "    support = support*len(dataset) \n",
    "    delete = [item for item in freq_items if freq_items[item] < support]\n",
    "    for item in delete: \n",
    "        del freq_items[item]\n",
    "    \n",
    "    return freq_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to obtain the frequent items: we show the dictionary (i.e., the count for each item), and then we use only the keys and put them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = 0.5\n",
    "freq_item = createL1(dataset, support)\n",
    "print(\"The frequent items with thier counts are:\\n\", freq_item)\n",
    "\n",
    "freq_item_keys = sorted(list(freq_item.keys()))\n",
    "print(\"\\n\\nThe frequent items (L1) are:\", freq_item_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive algorithm \n",
    "\n",
    "Assuming we did not do the first pass, but we would like to find the pairs directly, with a single pass. Let's compute how much memory we need (we will compare the value with the second pass of the Apriori algorithm).\n",
    "\n",
    "In ordert to count how many times a pair occurs, we use a dictionay, where the key is the pair. Since dictionary keys must be immutable, the pair are represented with a **tuple**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # used in computing the byte size\n",
    "\n",
    "def createL2_naive(dataset, support):\n",
    "    freq_itemset = {}\n",
    "    for basket in dataset:\n",
    "        sorted(basket)\n",
    "        len_basket = len(basket)\n",
    "        for i in range(len_basket):\n",
    "            for j in range(i+1, len_basket):\n",
    "                pair_tuple = (basket[i],basket[j])\n",
    "                if pair_tuple in freq_itemset:\n",
    "                    freq_itemset[pair_tuple] +=1\n",
    "                else:\n",
    "                    freq_itemset[pair_tuple] =1\n",
    "    # At this point, freq_itemset contains all the pairs:\n",
    "    # this is the maximum space used by this method\n",
    "    byte_size = sys.getsizeof(freq_itemset)\n",
    "                    \n",
    "    # remove non frequent items; len(data) returns the number of baskets\n",
    "    support = support*len(dataset) \n",
    "    delete = [item for item in freq_itemset if freq_itemset[item] < support]\n",
    "    for item in delete: \n",
    "        del freq_itemset[item]\n",
    "    \n",
    "    return freq_itemset, byte_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the frequent pairs (we show the dictionary, i.e., the count for each pair), along with the maximum memory used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pair_naive, naive_size = createL2_naive(dataset, support)\n",
    "print(\"The frequent pairs with thier counts are:\\n\", freq_pair_naive)\n",
    "print(\"The memory used with the naive approach is (bytes): \", naive_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second pass\n",
    "\n",
    "With the second pass, for each basket, we first remove non frequent items, then we build all the pairs. In addition to the dataset, we need to pass the L1 list (a list of item idenfitiers, not their counts).\n",
    "\n",
    "In ordert to count how many times a pair occurs, we use a dictionay, where the key is the pair. Since dictionary keys must be immutable, the pair are represented with a **tuple**. \n",
    "\n",
    "With this pass, we directly compute **L2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createL2(dataset, L1, support):\n",
    "    freq_itemset = {}\n",
    "    for basket in dataset:\n",
    "        # remove non freq. items\n",
    "        filtered_basket = []\n",
    "        for item in basket:\n",
    "            if item in L1:\n",
    "                filtered_basket.append(item)\n",
    "        # generate couples\n",
    "        sorted(filtered_basket)\n",
    "        len_basket = len(filtered_basket)\n",
    "        for i in range(len_basket):\n",
    "            for j in range(i+1, len_basket):\n",
    "                pair_tuple = (filtered_basket[i],filtered_basket[j])\n",
    "                if pair_tuple in freq_itemset:\n",
    "                    freq_itemset[pair_tuple] +=1\n",
    "                else:\n",
    "                    freq_itemset[pair_tuple] =1\n",
    "    # The freq_itemset contains all the pairs built from freq items:\n",
    "    # this is the maximum space used by this method\n",
    "    byte_size = sys.getsizeof(freq_itemset)\n",
    "                    \n",
    "    # remove non frequent itemset; \n",
    "    # len(data) returns the number of baskets\n",
    "    support = support*len(dataset) \n",
    "    delete = [item for item in freq_itemset if freq_itemset[item] < support]\n",
    "    for item in delete: \n",
    "        del freq_itemset[item]\n",
    "    \n",
    "    return freq_itemset, byte_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to obtain the frequent pairs (we show the dictionary, i.e., the count for each pair)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pair, apriori_size = createL2(dataset, freq_item_keys, support)\n",
    "print(\"The frequent pairs with thier counts are:\\n\", freq_pair)\n",
    "print(\"The memory used with the apriori approach is (bytes): \", apriori_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third pass\n",
    "\n",
    "Now it is possible to find the triples, which is left as exercise.\n",
    "\n",
    "Triples (**C3**) can be build starting, as input, from L1 and L2 (only the keys, not the whole dictionary).\n",
    "\n",
    "```python\n",
    "def createL3(dataset, L2, L1, support):\n",
    "    freq_itemset = {}\n",
    "    for basket in dataset:\n",
    "        # remove non freq. items\n",
    "        filtered_basket = []\n",
    "        for item in basket:\n",
    "            if item in L1:\n",
    "                filtered_basket.append(item)\n",
    "        # generate triples, but only if the \n",
    "        # possible couples are frequent \n",
    "        ...         \n",
    "    # remove non frequent itemset; \n",
    "    ...\n",
    "    return freq_itemset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q1\n",
    "<div class=\"alert alert-info\">\n",
    "Using the skeleton provided above, implement the function createL3( )\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q2\n",
    "<div class=\"alert alert-info\">\n",
    "Find the triples and the amount of memory used to compute such a result\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
