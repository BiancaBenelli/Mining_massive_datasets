{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding similar items\n",
    "\n",
    "In this notebook we will focus on fiding similar documents. We will start with finding the shingles of each document, then we will find the MinHash. As for the last step, LSH, we will use a library.\n",
    "\n",
    "We assume that the input is a file that contains, for each line, a document. The first word of each line is the identifier of the document. An example is:\n",
    "\n",
    "```text\n",
    "t980 A man was shot dead and fifteen others injured...\n",
    "t1088 Russian Prime Minister Viktor Chernomyrdin on Thursday proposed...\n",
    "t1233 Michael Johnson, who improved his own indoor 400m world record...\n",
    "...\n",
    "```\n",
    "\n",
    "The other input will the the similarity threshold used to differentiate similar/not similar items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "In this specific case, we are going to read the line and separate the first word (identifier) from the rest of the line. We then return two lists, one with the identifiers, the other with the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    doc_names = []\n",
    "    actual_doc = []\n",
    "    raw_lines = open(filename, 'r').read().splitlines()\n",
    "    for line in raw_lines:\n",
    "        # Split the line into words\n",
    "        words = line.split(\" \") \n",
    "  \n",
    "        # The doc ID is the first word \n",
    "        doc_ID = words[0]\n",
    "  \n",
    "        # Add doc ID to the list of document IDs\n",
    "        doc_names.append(doc_ID)\n",
    "        \n",
    "        # Remove the first word, and build back the line\n",
    "        del words[0]  \n",
    "        filtered_line = \" \".join(words)\n",
    "        actual_doc.append(filtered_line)\n",
    "    \n",
    "    return doc_names, actual_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide two datasets:\n",
    "- \"2-articles_100.txt\": a small dataset with 100 docs;\n",
    "- \"2-articles_1000.txt\": a larger dataset with 1000 docs.\n",
    "\n",
    "On Colab, remember to mount your Drive\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "input_file = \"/content/drive/My Drive/...\"\n",
    "```\n",
    "\n",
    "Otherwise, simply load your chosen file (we start with the small one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./2-articles_100.txt\"\n",
    "\n",
    "doc_names, docs_to_analyze = load_data(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the shingles\n",
    "\n",
    "For the shingles, we consider as token each character, and the size of the shingle is given as input parameter. Each shingle is hashed into a 32-bit integer, and we will use a special library for this, which must be imported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hashlib\n",
    "\n",
    "def compute_shingles(doc_names, docs_to_analyze, shingle_lenght=9):\n",
    "    docs_shingle_sets = {}\n",
    "    total_shingles = 0\n",
    "    num_docs = len(docs_to_analyze)\n",
    "    for i in range(num_docs):\n",
    "        # Consider one doc at a time\n",
    "        doc = docs_to_analyze[i]\n",
    "        \n",
    "        # Set for all of the unique shingle IDs present in the current doc\n",
    "        shingles_in_doc = set()\n",
    "\n",
    "        for index in range(len(doc) - shingle_lenght):\n",
    "            shingle = doc[index:index+shingle_lenght]\n",
    "            # Hash the shingle to a 32-bit integer.\n",
    "            crc = int(hashlib.sha256(shingle.encode('utf-8')).hexdigest(), 16) % 2**32\n",
    "\n",
    "            # Add the hash to the list of shingles \n",
    "            shingles_in_doc.add(crc)\n",
    "  \n",
    "        # Store the completed list of shingles for this document in the dictionary\n",
    "        doc_ID = doc_names[i]\n",
    "        docs_shingle_sets[doc_ID] = shingles_in_doc\n",
    "  \n",
    "        # Count the number of shingles across all documents.\n",
    "        total_shingles = total_shingles + (len(doc) - shingle_lenght)\n",
    "\n",
    "    return docs_shingle_sets, total_shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to compute the shingles. We consider a shingle lenght of 9 characters. We check also how mush time it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingling 100 docs took 0.76 sec.\n",
      "\n",
      "Average shingles per doc: 1544.70\n"
     ]
    }
   ],
   "source": [
    "shingle_lenght = 9\n",
    "t0 = time.time()\n",
    "docs_shingle_sets, total_shingles = compute_shingles(doc_names, \n",
    "                                                     docs_to_analyze, \n",
    "                                                     shingle_lenght)\n",
    "\n",
    "print('Shingling ' + str(len(docs_to_analyze)) + ' docs took %.2f sec.\\n' % (time.time() - t0))\n",
    " \n",
    "print('Average shingles per doc: %.2f' % (total_shingles / len(docs_to_analyze)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarities using shingles\n",
    "\n",
    "We use a naive computation where we compare each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard_sim_naive(doc_names, docs_shingle_sets, j_threshold):\n",
    "    similar_pairs = {}\n",
    "    num_docs = len(doc_names)\n",
    "    for i in range(num_docs):\n",
    "        # Shingles for document i\n",
    "        s_i = docs_shingle_sets[doc_names[i]]\n",
    "        for j in range(i+1, num_docs):\n",
    "            # Shingles for document j\n",
    "            s_j = docs_shingle_sets[doc_names[j]]\n",
    "        \n",
    "            jaccard_sim = (len(s_i.intersection(s_j)) / len(s_i.union(s_j)))\n",
    "            if jaccard_sim >= j_threshold:\n",
    "                similar_pairs[(doc_names[i],doc_names[j])] = jaccard_sim\n",
    "    \n",
    "    return similar_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the similar pair and see how much time it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating all Jaccard Similarities took 1.73 sec\n",
      "\n",
      "t980 and t2023 has similarity 0.9840166782487839\n",
      "t1088 and t5015 has similarity 0.9869366427171783\n",
      "t1297 and t4638 has similarity 0.9849869451697127\n",
      "t1768 and t5248 has similarity 0.9857050032488629\n",
      "t1952 and t3495 has similarity 0.9825418994413407\n"
     ]
    }
   ],
   "source": [
    "j_threshold = 0.6\n",
    "t0 = time.time()\n",
    "\n",
    "similar_pairs = Jaccard_sim_naive(doc_names, docs_shingle_sets, j_threshold)\n",
    "\n",
    "print(\"Calculating all Jaccard Similarities took %.2f sec\\n\"% (time.time() - t0))\n",
    "\n",
    "for pair in similar_pairs.keys():\n",
    "    print(pair[0], \"and\", pair[1], \"has similarity\", similar_pairs[pair])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the MinHashes\n",
    "\n",
    "We now transform each shingle into the corresponding MinHash.\n",
    "\n",
    "The method below assumes that the shingles IDs are coded into a 32-bit integer, so there are some hard-coded constants. A general method would take these constants as input.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def compute_MinHases(doc_names, docs_shingle_sets, num_hashes, seed = 289386372):\n",
    "    # Hard-coded contants\n",
    "    max_shingle_ID = 2**32-1\n",
    "    next_prime = 4294967311 # next largest prime number above 'max_shingle_ID'\n",
    "\n",
    "    # Set the seed in the random number genertor\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # The random hash function will take the form of:\n",
    "    #   h(x) = (a*x + b) % c\n",
    "    # Where 'x' is the input value, 'a' and 'b' are random coefficients, \n",
    "    # and 'c' is a prime number greater than max_shingle_ID.\n",
    "\n",
    "    # We compute the coefficients: the \"random.sample(N,k)\" returns the first k elements\n",
    "    # of a random permutation of set of N integers\n",
    "    coeffA = random.sample(range(max_shingle_ID), num_hashes)\n",
    "    coeffB = random.sample(range(max_shingle_ID), num_hashes)\n",
    "    \n",
    "    # Rather than generating a random permutation of all possible shingles, \n",
    "    # we'll just hash the IDs of the shingles that are *actually in the document*,\n",
    "    # then take the lowest resulting hash code value. This corresponds to the index \n",
    "    # of the first shingle that you would have encountered in the random order.\n",
    "\n",
    "    all_signatures = {}\n",
    "    \n",
    "    for doc_ID in doc_names:\n",
    "        # Get the shingle set for this document.\n",
    "        shingle_set = docs_shingle_sets[doc_ID]\n",
    "  \n",
    "        # The resulting minhash signature for this document. \n",
    "        signature = []\n",
    "  \n",
    "        for i in range(num_hashes):\n",
    "            # For each of the shingles actually in the document, calculate its hash code\n",
    "            # using hash function 'i'. \n",
    "    \n",
    "            # Track the lowest hash ID seen. Initialize 'minHashCode' to be greater than\n",
    "            # the maximum possible value output by the hash.\n",
    "            min_hash_code = next_prime + 1\n",
    "    \n",
    "            for shingle_ID in shingle_set:\n",
    "                hash_code = (coeffA[i] * shingle_ID + coeffB[i]) % next_prime \n",
    "                if hash_code < min_hash_code:\n",
    "                    min_hash_code = hash_code\n",
    "\n",
    "            # Add the smallest hash code value as component number 'i' of the signature.\n",
    "            signature.append(min_hash_code)\n",
    "  \n",
    "        # Store the MinHash signature for this document.\n",
    "        all_signatures[doc_ID] = signature\n",
    "        \n",
    "    return all_signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the MinHashes and see how much time it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating MinHash signatures took 8.89 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_hashes = 100\n",
    "\n",
    "t0 = time.time()\n",
    "docs_minhash = compute_MinHases(doc_names, docs_shingle_sets, num_hashes)\n",
    "\n",
    "print('Generating MinHash signatures took %.2f sec\\n' % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarities using MinHash\n",
    "\n",
    "We compute the Jaccard similarities between pair with the naive method, but using the signatures instead of the whole set of shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard_sim_minhash_naive(doc_names, docs_minhash, j_threshold):\n",
    "    similar_pairs = {}\n",
    "    num_docs = len(doc_names)\n",
    "    for i in range(num_docs):\n",
    "        # Shingles for document i\n",
    "        s_i = docs_minhash[doc_names[i]]\n",
    "        num_hashes = len(s_i)\n",
    "        for j in range(i+1, num_docs):\n",
    "            # Shingles for document j\n",
    "            s_j = docs_minhash[doc_names[j]]\n",
    "\n",
    "            count = 0\n",
    "            for k in range(num_hashes):\n",
    "                count = count + (s_i[k] == s_j[k])\n",
    "\n",
    "            jaccard_sim = (count / num_hashes)\n",
    "            if jaccard_sim >= j_threshold:\n",
    "                similar_pairs[(doc_names[i],doc_names[j])] = jaccard_sim\n",
    "    \n",
    "    return similar_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the similar pair with the MinHash and see how much time it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating all Jaccard Similarities with MinHash took 0.13 sec\n",
      "\n",
      "t980 and t2023 has similarity 0.99\n",
      "t1088 and t5015 has similarity 1.0\n",
      "t1297 and t4638 has similarity 0.97\n",
      "t1768 and t5248 has similarity 0.99\n",
      "t1952 and t3495 has similarity 0.98\n"
     ]
    }
   ],
   "source": [
    "j_threshold = 0.6\n",
    "t0 = time.time()\n",
    "\n",
    "similar_pairs_minhash = Jaccard_sim_minhash_naive(doc_names, docs_minhash, j_threshold)\n",
    "\n",
    "print(\"Calculating all Jaccard Similarities with MinHash took %.2f sec\\n\"% (time.time() - t0))\n",
    "\n",
    "for pair in similar_pairs_minhash.keys():\n",
    "    print(pair[0], \"and\", pair[1], \"has similarity\", similar_pairs_minhash[pair])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a LSH library\n",
    "\n",
    "Since LSH is useful tool, there exist libraries that implement LSH, so that one can simply import the library and use the function.\n",
    "\n",
    "One example is the MinHash and the LSH implementation from the SNAPY library:  \n",
    "https://pypi.org/project/snapy/\n",
    "\n",
    "In order to use is, we need to install the library, if not already done before\n",
    "\n",
    "```python\n",
    "pip install snapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q1\n",
    "<div class=\"alert alert-info\">\n",
    "Using the MinHash and the LSH implementation from the SNAPY library (see the documentation on the link provided above) compute the minhash and LSH of the dataset used so far (2-articles_100.txt).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q2\n",
    "<div class=\"alert alert-info\">\n",
    "Compute the runing time for the operations in Q1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q3\n",
    "<div class=\"alert alert-info\">\n",
    "Print the similar pairs with Jaccard similarity at least 0.6 and compare the result with the one obtained before. Why the list is longer? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q4\n",
    "<div class=\"alert alert-info\">\n",
    "Repeat the computation with a larger dataset (2-articles_1000.txt)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This notebook has been inspired by:  \n",
    "- McCormick, C. (2015, June 12). MinHash Tutorial with Python Code. Retrieved from http://www.mccormickml.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
