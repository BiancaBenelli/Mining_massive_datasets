{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset = set of documents\n",
    "My focus is on showing the benefits of using LSH in sorting the documents into buckets <- showing how faster it is of a Naive approach (= compare all documents). The steps:\n",
    "Documents -> shingles representation in a high dimensional vectors -> use these vectors to compute the Jaccard similarity -> from high dim to low dim by computing the Min Hash (sim by signature) -> use LSH from Snapy library and compare efficiency.\n",
    "I compute the speed of each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding similar items\n",
    "\n",
    "In this notebook we will focus on fiding similar documents. We will start with finding the shingles of each document, then we will find the MinHash. As for the last step, LSH, we will use a library.\n",
    "\n",
    "We assume that the input is a file that contains, for each line, a document. The first word of each line is the identifier of the document. An example is:\n",
    "\n",
    "```text\n",
    "t980 A man was shot dead and fifteen others injured...\n",
    "t1088 Russian Prime Minister Viktor Chernomyrdin on Thursday proposed...\n",
    "t1233 Michael Johnson, who improved his own indoor 400m world record...\n",
    "...\n",
    "```\n",
    "\n",
    "The other input will the the similarity threshold used to differentiate similar/not similar items. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "The dataset is already pre-processed, now each line is a whole document (easier formatting to use): first word is an identifier of the doc and then the rest is a sorted string of words of the doc in una single line.\n",
    "\n",
    "In this specific case, we are going to read the line and separate the first word (identifier) from the rest of the line. We then return two lists, one with the identifiers, the other with the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the data\n",
    "def load_data(filename):\n",
    "    # Define two empy lists\n",
    "    doc_names = [] # identifier of the doc -> I will truly need it at the end to display the results\n",
    "    actual_doc = [] # test of the doc\n",
    "    raw_lines = open(filename, 'r').read().splitlines()\n",
    "    for line in raw_lines:\n",
    "        # Read the file line by line, each split considering the space\n",
    "        words = line.split(\" \") # Split the line into words\n",
    "  \n",
    "        # The identifier of the doc is the first word \n",
    "        doc_ID = words[0]\n",
    "        # Add doc ID to the list of document IDs\n",
    "        doc_names.append(doc_ID)\n",
    "        \n",
    "        # Remove the first word, and build back the line to add to actual_doc\n",
    "        del words[0]  \n",
    "        filtered_line = \" \".join(words)\n",
    "        actual_doc.append(filtered_line)\n",
    "        # Since append add at the end, the order of the doc is mantained\n",
    "    \n",
    "    return doc_names, actual_doc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide two datasets:\n",
    "- \"2_articles_100.txt\": a small dataset with 100 docs;\n",
    "- \"2_articles_1000.txt\": a larger dataset with 1000 docs.\n",
    "\n",
    "On Colab, remember to mount your Drive\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "input_file = \"/content/drive/My Drive/...\"\n",
    "```\n",
    "\n",
    "Otherwise, simply load your chosen file (we start with the small one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./2_articles_100.txt\" # small dataset of 100 docs (fast)\n",
    "\n",
    "doc_names, docs_to_analyze = load_data(input_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the shingles\n",
    "\n",
    "For the shingles, we consider as token each character, and the size of the shingle is given as input parameter. Each shingle is hashed into a 32-bit integer, and we will use a special library for this, which must be imported.\n",
    "\n",
    "I now focus on the single document (= string with diff words insid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hashlib\n",
    "\n",
    "# Define the number of characters per shingles (here is 9 by default, I can change it to tune the results) -> compute the hash function and get a number\n",
    "# -> put the number in a set (which assumes its elements are distinct, that's why I use it)\n",
    "# I'm counting the total numbers of shingles\n",
    "def compute_shingles(doc_names, docs_to_analyze, shingle_lenght = 9): # usually for love doc I have 8/9, for smaller doc 4/5 (in english this counts 3 words - end, whole, start)\n",
    "    docs_shingle_sets = {} # set made up of the numbers obtained by the h.f.\n",
    "    total_shingles = 0 # numbers of shingles in the whole dataset\n",
    "    num_docs = len(docs_to_analyze) # length of a doc = number of characters in it\n",
    "    for i in range(num_docs):\n",
    "        # Consider one doc at a time\n",
    "        doc = docs_to_analyze[i]\n",
    "        \n",
    "        # Set for all of the unique shingle IDs present in the current doc\n",
    "        shingles_in_doc = set()\n",
    "\n",
    "        # I create shingles by considering a sliding window\n",
    "        # The index goes from 0 to the length of the doc - shingle_length (ex. len = 100 and shingle_length = 9: first shingle from 0 to 8, second from 1 to 9, etc., until 91)\n",
    "        for index in range(len(doc) - shingle_lenght):\n",
    "            # I can do this since the doc are strings\n",
    "            shingle = doc[index: index + shingle_lenght] # select the substring\n",
    "\n",
    "            # Hash the shingle to a 32-bit integer: translates the sequence of characters into a 32-bit integer\n",
    "            crc = int(hashlib.sha256(shingle.encode('utf-8')).hexdigest(), 16) % 2**32\n",
    "            # Add the hash to the set of shingles \n",
    "            shingles_in_doc.add(crc)\n",
    "  \n",
    "        # Store the completed list of shingles for this document in the dictionary\n",
    "        doc_ID = doc_names[i]\n",
    "        docs_shingle_sets[doc_ID] = shingles_in_doc\n",
    "  \n",
    "        # Count the number of shingles across all documents.\n",
    "        total_shingles = total_shingles + (len(doc) - shingle_lenght)\n",
    "\n",
    "    return docs_shingle_sets, total_shingles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to compute the shingles, considering a shingle lenght of 9 characters. I have transformed my documents from strings to vectors containing the numbers that represnt the documnet itself.\n",
    "\n",
    "We check also how mush time it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingling 100 docs took 0.29 sec.\n",
      "\n",
      "Average shingles per doc: 1544.70\n"
     ]
    }
   ],
   "source": [
    "shingle_lenght = 9\n",
    "t0 = time.time()\n",
    "docs_shingle_sets, total_shingles = compute_shingles(doc_names, docs_to_analyze, shingle_lenght)\n",
    "\n",
    "print('Shingling ' + str(len(docs_to_analyze)) + ' docs took %.2f sec.\\n' % (time.time() - t0))\n",
    " \n",
    "print('Average shingles per doc: %.2f' % (total_shingles / len(docs_to_analyze))) # average shingles in a set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarities using shingles\n",
    "\n",
    "We use a naive computation where we compare each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold = 2 documents are considered similar if they share at least x% elements (= numbers) -> share x% of shingles\n",
    "def Jaccard_sim_naive(doc_names, docs_shingle_sets, j_threshold):\n",
    "    similar_pairs = {}\n",
    "    num_docs = len(doc_names)\n",
    "    # I compare each doc with the others -> first for cycle is for passing all documents\n",
    "    for i in range(num_docs):\n",
    "        # Retrieve the shingles for document i -> vector\n",
    "        s_i = docs_shingle_sets[doc_names[i]]\n",
    "        # Second for loop is for document after (i+1) until the end (I don't need to go back since I have already done the comparison)\n",
    "        for j in range(i+1, num_docs):\n",
    "            # Retrieve shingles for document j -> vector\n",
    "            s_j = docs_shingle_sets[doc_names[j]]\n",
    "        \n",
    "            # Compute the Jaccard similarity: since the shingles are sets, I use the functions .intesection and .union\n",
    "            jaccard_sim = (len(s_i.intersection(s_j)) / len(s_i.union(s_j))) # count how many members are in the intersection/union\n",
    "            # if sim >= threshold, then the doc are similar -> I record the pair and the corresponding similarity\n",
    "            if jaccard_sim >= j_threshold:\n",
    "                similar_pairs[(doc_names[i], doc_names[j])] = jaccard_sim\n",
    "    \n",
    "    return similar_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the similar pair and see how much time it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating all Jaccard Similarities took 0.90 sec\n",
      "\n",
      "t980 and t2023 has similarity 0.9840166782487839\n",
      "t1088 and t5015 has similarity 0.9869366427171783\n",
      "t1297 and t4638 has similarity 0.9849869451697127\n",
      "t1768 and t5248 has similarity 0.9857050032488629\n",
      "t1952 and t3495 has similarity 0.9825418994413407\n"
     ]
    }
   ],
   "source": [
    "j_threshold = 0.6 #60%\n",
    "t0 = time.time()\n",
    "\n",
    "similar_pairs = Jaccard_sim_naive(doc_names, docs_shingle_sets, j_threshold) # I consider the shingles, not the whole doc!\n",
    "\n",
    "print(\"Calculating all Jaccard Similarities took %.2f sec\\n\"% (time.time() - t0))\n",
    "\n",
    "# print the similar documents\n",
    "for pair in similar_pairs.keys():\n",
    "    print(pair[0], \"and\", pair[1], \"has similarity\", similar_pairs[pair])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a naive approach that compute the similarity between ALL documents, it works but it's not too slow only because the dataset is small (100 docs). With a bigger dataset, I have to change strategy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the MinHashes\n",
    "\n",
    "We now transform each shingle into the corresponding MinHash (= creata a set of signature) and compare efficiency with above. The set of signature is much smaller than the set of shingles (100 elements vs 1500 elements).\n",
    "\n",
    "The method below assumes that the shingles IDs are coded into a 32-bit integer, so there are some hard-coded constants. A general method would take these constants as input.   \n",
    "\n",
    "At the end I check if the translation kept the property of similarity, by comparing the current results with the aboves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Started from a document aas a string -> create shingles by considering set of characters (an hash function translate the set in a number) -> the\n",
    "# set of number is transalted in a binary vector -> apply a permutation to the binary vector -> the minHash (=signature) of a permutation is the index\n",
    "# where I find the first 1 (idc where the zeros go) -> repeat for others permutations, for all 100 permutaions -> minHash is made by a sequence of\n",
    "# indexes, which is smaller :)\n",
    "# Applying first permutation is equivalent to using an hash function to transform the the shingle, then choosing the smallest value to put into the\n",
    "# signature - it's the hash function h(x) -> repeat the process until I fill the signature (100 times)\n",
    "\n",
    "# It's more complex, since I use permutation of columns and find the integer of the smallest number inside that column\n",
    "# I transalte the set of numbers (with no order) into binary vectors (where the values of the set are indexes) -> apply a random\n",
    "# permutation -> find the index with the first 1 = findthe index of the smallest 1 -> record in the signature (= Min Hash).\n",
    "def compute_MinHashes(doc_names, docs_shingle_sets, num_hashes, seed = 289386372):\n",
    "    # Hard-coded contants\n",
    "    max_shingle_ID = 2**32-1\n",
    "    next_prime = 4294967311 # next largest prime number above 'max_shingle_ID'\n",
    "\n",
    "    # Set the seed in the random number genertor\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # PERMUTATION COMPUTED AS A HASH FUNCTION \n",
    "    # Permutation are easily computed by using a hash function with coefficients 'a' and 'b' ('c' is a prime number greater than the maximum shingle ID)\n",
    "    # The random hash function will take the form of:\n",
    "    #   h(x) = (a*x + b) % c\n",
    "    # Where 'x' is the input value, 'a' and 'b' are random coefficients, and 'c' is a prime number greater than max_shingle_ID.\n",
    "    # Here 'a' and 'b' are vector with 100 values\n",
    "\n",
    "    # We compute the coefficients: the \"random.sample(N, k)\" returns the first k elements\n",
    "    # of a random permutation of set of N integers\n",
    "    coeffA = random.sample(range(max_shingle_ID), num_hashes) # sequences with 100 numer of hashes of random numbers\n",
    "    coeffB = random.sample(range(max_shingle_ID), num_hashes) # sequences with 100 numer of hashes of random numbers\n",
    "    \n",
    "    # Rather than generating a random permutation of all possible shingles, \n",
    "    # we'll just hash the IDs of the shingles that are *actually in the document*,\n",
    "    # then take the lowest resulting hash code value. This corresponds to the index \n",
    "    # of the first shingle that you would have encountered in the random order.\n",
    "\n",
    "    all_signatures = {}\n",
    "    \n",
    "    # For cycle to consider each document -> Focus on single documents\n",
    "    for doc_ID in doc_names:\n",
    "        # Get the shingle set for this document\n",
    "        shingle_set = docs_shingle_sets[doc_ID]\n",
    "  \n",
    "        # The resulting minhash signature for this document. \n",
    "        signature = []\n",
    "\n",
    "        # For each shingle in the shingle set I compute its hash code with the hash functionswith parameters 'a' and 'b' (which change every iteraction)\n",
    "        for i in range(num_hashes):\n",
    "            # For each of the shingles actually in the document, calculate its hash code\n",
    "            # using hash function 'i'. \n",
    "    \n",
    "            # Track the lowest hash ID seen. Initialize 'minHashCode' to be greater than\n",
    "            # the maximum possible value output by the hash.\n",
    "            min_hash_code = next_prime + 1\n",
    "    \n",
    "            # Change the order of the shingles (= perm) -> index i become index j -> track where index j go and track the minimum to put into the signature\n",
    "            # shingle_ID is a number\n",
    "            for shingle_ID in shingle_set: # for each shingles in the shingle set\n",
    "                hash_code = (coeffA[i] * shingle_ID + coeffB[i]) % next_prime # shingle_ID is a number\n",
    "                if hash_code < min_hash_code:\n",
    "                    # I keep track of the min value of hash code\n",
    "                    min_hash_code = hash_code\n",
    "\n",
    "            # At the end, add the smallest hash code value as component number 'i' of the signature.\n",
    "            signature.append(min_hash_code)\n",
    "  \n",
    "        # Store the MinHash signature for this document.\n",
    "        all_signatures[doc_ID] = signature\n",
    "        \n",
    "    return all_signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the MinHashes and see how much time it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating MinHash signatures took 6.58 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_hashes = 100\n",
    "\n",
    "t0 = time.time()\n",
    "docs_minhash = compute_MinHashes(doc_names, docs_shingle_sets, num_hashes)\n",
    "\n",
    "# It's a bit slow because I go through all elements of all documents\n",
    "print('Generating MinHash signatures took %.2f sec\\n' % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarities using MinHash\n",
    "\n",
    "We compute the Jaccard similarities between pair with the naive method, but using the signatures instead of the whole set of shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I compute the Jaccard sim using the Min Hash, but I consider only the signatures and not all the shingle set\n",
    "def Jaccard_sim_minhash_naive(doc_names, docs_minhash, j_threshold):\n",
    "    similar_pairs = {}\n",
    "    num_docs = len(doc_names)\n",
    "    for i in range(num_docs):\n",
    "        # Shingles for document i\n",
    "        s_i = docs_minhash[doc_names[i]]\n",
    "        num_hashes = len(s_i)\n",
    "        for j in range(i+1, num_docs):\n",
    "            # Shingles for document j\n",
    "            s_j = docs_minhash[doc_names[j]]\n",
    "\n",
    "            count = 0\n",
    "            for k in range(num_hashes):\n",
    "                count = count + (s_i[k] == s_j[k])\n",
    "\n",
    "            jaccard_sim = (count / num_hashes)\n",
    "            if jaccard_sim >= j_threshold:\n",
    "                similar_pairs[(doc_names[i],doc_names[j])] = jaccard_sim\n",
    "    \n",
    "    return similar_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the similar pair with the MinHash and see how much time it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating all Jaccard Similarities with MinHash took 0.13 sec\n",
      "\n",
      "t980 and t2023 has similarity 0.99\n",
      "t1088 and t5015 has similarity 1.0\n",
      "t1297 and t4638 has similarity 0.97\n",
      "t1768 and t5248 has similarity 0.99\n",
      "t1952 and t3495 has similarity 0.98\n"
     ]
    }
   ],
   "source": [
    "j_threshold = 0.6\n",
    "t0 = time.time()\n",
    "\n",
    "similar_pairs_minhash = Jaccard_sim_minhash_naive(doc_names, docs_minhash, j_threshold)\n",
    "\n",
    "print(\"Calculating all Jaccard Similarities with MinHash took %.2f sec\\n\"% (time.time() - t0))\n",
    "\n",
    "# The results are close enough to the ones above (I don't have a big differnce due to the small size of the dataset)\n",
    "for pair in similar_pairs_minhash.keys():\n",
    "    print(pair[0], \"and\", pair[1], \"has similarity\", similar_pairs_minhash[pair])\n",
    "# It's much faster!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity is computed using the signatures -> I get the same results + mantain the properties of similarity.\n",
    "\n",
    "The computation of the signature is long, but it's done only once! If I introduce a new doc, I compute just its siganture and the comparison with other docs is fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a LSH library\n",
    "\n",
    "Since LSH is useful tool, there exist libraries that implement LSH, so that one can simply import the library and use the function.\n",
    "\n",
    "One example is the MinHash and the LSH implementation from the SNAPY library:  \n",
    "https://pypi.org/project/snapy/\n",
    "\n",
    "In order to use is, we need to install the library, if not already done before\n",
    "\n",
    "```python\n",
    "pip install snapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q1\n",
    "<div class=\"alert alert-info\">\n",
    "Using the MinHash and the LSH implementation from the SNAPY library (see the documentation on the link provided above) compute the minhash and LSH of the dataset used so far (2-articles_100.txt).\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents grouped together into buckets thanks to LSH -> compare only docs in the same bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "import snapy\n",
    "print(dir(snapy))\n",
    "# permutation = size of minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package snapy:\n",
      "\n",
      "NAME\n",
      "    snapy\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    planet_gravity\n",
      "\n",
      "FILE\n",
      "    c:\\users\\330s-15ikb-w3ix\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\snapy\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(snapy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'snapy' has no attribute 'MinHash'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\330s-15ikb-w3ix\\Desktop\\UniVr\\Primo Anno\\Mining of Massive Datasets\\Mining_massive_datasets\\Mining_massive_datasets\\2_similar_iitems\\2_similar_items.ipynb Cell 30\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/330s-15ikb-w3ix/Desktop/UniVr/Primo%20Anno/Mining%20of%20Massive%20Datasets/Mining_massive_datasets/Mining_massive_datasets/2_similar_iitems/2_similar_items.ipynb#X55sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/330s-15ikb-w3ix/Desktop/UniVr/Primo%20Anno/Mining%20of%20Massive%20Datasets/Mining_massive_datasets/Mining_massive_datasets/2_similar_iitems/2_similar_items.ipynb#X55sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Create MinHash object.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/330s-15ikb-w3ix/Desktop/UniVr/Primo%20Anno/Mining%20of%20Massive%20Datasets/Mining_massive_datasets/Mining_massive_datasets/2_similar_iitems/2_similar_items.ipynb#X55sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m minhash \u001b[39m=\u001b[39m snapy\u001b[39m.\u001b[39mMinHash(content, n_gram\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, permutations\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, hash_bits\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'snapy' has no attribute 'MinHash'"
     ]
    }
   ],
   "source": [
    "import snapy\n",
    "\n",
    "content = [\n",
    "    'Jupiter is primarily composed of hydrogen with a quarter of its mass '\n",
    "    'being helium',\n",
    "    'Jupiter moving out of the inner Solar System would have allowed the '\n",
    "    'formation of inner planets.',\n",
    "    'A helium atom has about four times as much mass as a hydrogen atom, so '\n",
    "    'the composition changes when described as the proportion of mass '\n",
    "    'contributed by different atoms.',\n",
    "    'Jupiter is primarily composed of hydrogen and a quarter of its mass '\n",
    "    'being helium',\n",
    "    'A helium atom has about four times as much mass as a hydrogen atom and '\n",
    "    'the composition changes when described as a proportion of mass '\n",
    "    'contributed by different atoms.',\n",
    "    'Theoretical models indicate that if Jupiter had much more mass than it '\n",
    "    'does at present, it would shrink.',\n",
    "    'This process causes Jupiter to shrink by about 2 cm each year.',\n",
    "    'Jupiter is mostly composed of hydrogen with a quarter of its mass '\n",
    "    'being helium',\n",
    "    'The Great Red Spot is large enough to accommodate Earth within its '\n",
    "    'boundaries.'\n",
    "]\n",
    "\n",
    "labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "seed = 3\n",
    "\n",
    "\n",
    "# Create MinHash object.\n",
    "minhash = snapy.MinHash(content, n_gram=9, permutations=100, hash_bits=64, seed=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q2\n",
    "<div class=\"alert alert-info\">\n",
    "Compute the runing time for the operations in Q1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q3\n",
    "<div class=\"alert alert-info\">\n",
    "Print the similar pairs with Jaccard similarity at least 0.6 and compare the result with the one obtained before. Why the list is longer? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q4\n",
    "<div class=\"alert alert-info\">\n",
    "Repeat the computation with a larger dataset (2-articles_1000.txt)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This notebook has been inspired by:  \n",
    "- McCormick, C. (2015, June 12). MinHash Tutorial with Python Code. Retrieved from http://www.mccormickml.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
